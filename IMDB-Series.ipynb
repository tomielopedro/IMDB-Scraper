{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc286077dbf6f099",
   "metadata": {},
   "source": [
    "# Etapa I: Coleta de Dados com o Selenium\n",
    "    Coletar dados de 250 séries com as maiores avaliações do site `https://www.imdb.com/pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c65d31aa714f10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T12:19:14.459138Z",
     "start_time": "2025-09-29T12:11:38.300419Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from dataclasses import asdict\n",
    "\n",
    "# Configuração dos logs\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "\n",
    "def scrape_serie(serie_data):\n",
    "    \"\"\"Abre um navegador separado e coleta dados de uma série\"\"\"\n",
    "    driver = webdriver.Chrome()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    try:\n",
    "        driver.get(serie_data[\"link\"])\n",
    "        popularidade = wait.until(\n",
    "            EC.presence_of_element_located((\n",
    "                By.XPATH,\n",
    "                '//*[@id=\"__next\"]/main/div/section[1]/section/div[3]/section/section/div[2]/div[2]/div/div[3]/a/span/div/div[2]/div[1]'))\n",
    ").text\n",
    "\n",
    "        elenco_section = wait.until(\n",
    "            EC.presence_of_all_elements_located((\n",
    "                By.XPATH,\n",
    "                '//*[@id=\"__next\"]/main/div/section[1]/div/section/div/div[1]/section[5]'))\n",
    "        )\n",
    "        elenco_principal = [ator.text for ator in elenco_section]\n",
    "\n",
    "        return {\n",
    "            \"info\": serie_data[\"info\"],\n",
    "            \"link\": serie_data[\"link\"],\n",
    "            \"popularidade\": popularidade,\n",
    "            \"elenco_principal\": elenco_principal\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro em {serie_data['link']}: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# COLETA DOS LINKS\n",
    "# ----------------------\n",
    "driver = webdriver.Chrome()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "driver.get(\"https://www.imdb.com/pt/\")\n",
    "\n",
    "menu_icon = wait.until(\n",
    "    EC.element_to_be_clickable((By.XPATH, '//*[@id=\"imdbHeader-navDrawerOpen\"]'))\n",
    ")\n",
    "menu_icon.click()\n",
    "\n",
    "link_series_bem_avaliadas = wait.until(\n",
    "    EC.element_to_be_clickable((\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"imdbHeader\"]/div/aside[1]/div/div[2]/div/div[2]/div[1]/span/div/div/ul/a[2]'))\n",
    ")\n",
    "link_series_bem_avaliadas.click()\n",
    "time.sleep(2) # esperar todas as 250 series carregarem\n",
    "series = wait.until(\n",
    "    EC.presence_of_all_elements_located((\n",
    "        By.XPATH,\n",
    "        '//*[@id=\"__next\"]/main/div/div[3]/section/div/div[2]/div/ul/li'))\n",
    ")\n",
    "\n",
    "series_data = []\n",
    "for serie in series:\n",
    "    try:\n",
    "        info = serie.text.split(\"\\n\")\n",
    "        link = serie.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "        series_data.append({\"info\": info, \"link\": link})\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Erro ao extrair série: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# ----------------------\n",
    "# COLETA EM PARALELO\n",
    "# ----------------------\n",
    "logging.info(f\"Encontradas {len(series_data)} séries. Iniciando coleta em paralelo...\")\n",
    "\n",
    "start_time = time.time()\n",
    "dados_completos = []\n",
    "total = len(series_data)\n",
    "coletadas = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=5) as executor:  # até 5 navegadores em paralelo\n",
    "    futures = [executor.submit(scrape_serie, s) for s in series_data]\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "        coletadas += 1\n",
    "        if result:\n",
    "            dados_completos.append(result)\n",
    "            logging.info(\n",
    "                f\"Coletada: {result['info'][0]} \"\n",
    "                f\"({coletadas}/{total} - {coletadas/total*100:.1f}%)\"\n",
    "            )\n",
    "        else:\n",
    "            logging.warning(f\"Série falhou ({coletadas}/{total} - {coletadas/total*100:.1f}%)\")\n",
    "\n",
    "end_time = time.time()\n",
    "logging.info(f\"Coleta finalizada em {end_time - start_time:.2f} segundos.\")\n",
    "\n",
    "print(dados_completos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c31c49",
   "metadata": {},
   "source": [
    "# Etapa II: Tratamento de Dados\n",
    "    - Criar funções que possibilitam tratar as listas de dados\n",
    "    - Criar objetos utilizando essas listas\n",
    "    - Criar um arquivo Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cd102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-29T12:32:41.859377Z",
     "start_time": "2025-09-29T12:32:41.439593Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Classes\n",
    "# ==========================\n",
    "@dataclass\n",
    "class Ator:\n",
    "    nome: str\n",
    "    papel: str\n",
    "    n_ep: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {\n",
    "            \"nome\": self.nome,\n",
    "            \"personagem\": self.papel,\n",
    "            \"quantidade_episodios\": self.n_ep\n",
    "        }\n",
    "\n",
    "@dataclass\n",
    "class Serie:\n",
    "    nome: str\n",
    "    ordem: int\n",
    "    ano_estreia: int\n",
    "    ano_encerramento: int\n",
    "    episodios: int\n",
    "    faixa_etaria: int  # Pode ser número ou \"Livre\" == 0\n",
    "    avaliacao: float\n",
    "    link: str\n",
    "    popularidade: float\n",
    "    atores: list\n",
    "\n",
    "    def to_dict(self):\n",
    "        # Transforma o objeto em Dicionário\n",
    "        return {\n",
    "            \"titulo\": self.nome,\n",
    "            \"ordem\": self.ordem,\n",
    "            \"ano_estreia\": self.ano_estreia,\n",
    "            \"ano_encerramento\": self.ano_encerramento,\n",
    "            \"episodios\": self.episodios,\n",
    "            \"classificacao_indicativa\": self.faixa_etaria,\n",
    "            \"nota_imdb\": self.avaliacao,\n",
    "            \"link\": self.link,\n",
    "            \"popularidade\": self.popularidade,\n",
    "            \"elenco_principal\": self.atores\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Funções de tratamento\n",
    "# ==========================\n",
    "def tratar_string(string):\n",
    "    # Normaliza traço en dash\n",
    "    string = string.replace(\"–\", \"-\")\n",
    "    # Extrai anos\n",
    "    anos = re.findall(r\"\\d{4}\", string)\n",
    "    print('anosssss', anos)\n",
    "    # Compara se existe ano de extreia e fim\n",
    "    if len(anos) > 1:\n",
    "        ano_estreia = anos[0]\n",
    "        ano_encerramento = int(anos[1])\n",
    "    else:\n",
    "        ano_estreia = anos[0]\n",
    "        ano_encerramento = None\n",
    "        \n",
    "    # Inicializa\n",
    "    episodios = None\n",
    "    faixa = None\n",
    "\n",
    "    # Regex: pega número de episódios e a faixa etária\n",
    "    match = re.search(r\"(\\d+)\\s*episódios\\s*(\\S+)?\", string, re.IGNORECASE)\n",
    "    if match:\n",
    "        full_number = match.group(1)\n",
    "        episodios = int(full_number[-2:])\n",
    "        faixa_raw = match.group(2)\n",
    "        # verifica se a faixa é um numero se não for a faixa é \"Livre\" então recebe zero\n",
    "        if faixa_raw:\n",
    "            numeros = re.findall(r\"\\d+\", faixa_raw)  # pega todos os números\n",
    "            if numeros:\n",
    "                faixa = int(numeros[0])  # converte o primeiro número que encontrar\n",
    "            else:\n",
    "                faixa = 0    # se não tiver número, transforma em zero\n",
    "\n",
    "    return int(ano_estreia), ano_encerramento, episodios, faixa\n",
    "\n",
    "\n",
    "def tratar_elenco(texto):\n",
    "    resultados = []\n",
    "    if isinstance(texto, list):\n",
    "        texto = \"\\n\".join(texto)\n",
    "\n",
    "    padrao = re.compile(\n",
    "        r\"([^\\n]+)\\n\"            # Ator\n",
    "        r\"([^\\n]+)\\n\"            # Personagem \n",
    "        r\"(\\d+)\\s*episódio[s]?\", # Episódios\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for ator, personagem, ep in padrao.findall(texto):\n",
    "        # adiciona a lista um objeto do tipo Atores\n",
    "        resultados.append(Ator(\n",
    "                ator.strip().replace('…', ''),\n",
    "                personagem.strip(),\n",
    "                int(ep)).to_dict()\n",
    "        )\n",
    "    return resultados\n",
    "\n",
    "\n",
    "def tratamento_geral(dicionario: dict):\n",
    "    # --Função para o tratamento geral dos dados-- #\n",
    "    for key, item in dicionario.items():\n",
    "        # Trata a primeira parte dos dados: 'info' (nome, ano, ep, faixa e avaliação)\n",
    "        if key == 'info':\n",
    "            nome_ordem = item[0].split('.', 1)\n",
    "            nome = nome_ordem[1].strip()\n",
    "            ordem = int(nome_ordem[0])\n",
    "            avaliacao = item[2]\n",
    "            avaliacao = float(avaliacao.replace(\",\", \".\"))\n",
    "            ano_estreia, ano_encerramento, episodio, faixa = tratar_string(item[1])\n",
    "            faixa = int(faixa)\n",
    "            \n",
    "        # Trata o link\n",
    "        elif key == 'link':\n",
    "            link = item\n",
    "\n",
    "        # Trata a popularidade\n",
    "        elif key == 'popularidade':\n",
    "            pop = float(item)\n",
    "\n",
    "        # Trata a lista de Atores\n",
    "        elif key == 'elenco_principal':\n",
    "            lista_de_atores = tratar_elenco(item)\n",
    "    # Retorna um objeto do tipo serie\n",
    "    return Serie(\n",
    "        nome,\n",
    "        ordem,\n",
    "        ano_estreia,\n",
    "        ano_encerramento,\n",
    "        episodio,\n",
    "        faixa,\n",
    "        avaliacao,\n",
    "        link,\n",
    "        pop,\n",
    "        lista_de_atores\n",
    "    )\n",
    "\n",
    "\n",
    "def covert_json(lista):\n",
    "    # Função que converte a lista em dicionario e cria um arquivo json ordenado\n",
    "    lista_dicts = [s.to_dict() for s in lista]\n",
    "    ordenado = sorted(lista_dicts, key=lambda x: int(x[\"ordem\"]))\n",
    "    \n",
    "    with open(\"series.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(ordenado, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Execução\n",
    "# ==========================\n",
    "lista_de_obj = list(map(tratamento_geral, dados_completos))\n",
    "covert_json(lista_de_obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06c6a4",
   "metadata": {},
   "source": [
    "# *Bônus*\n",
    "## Teste com BeautifulSoup\n",
    "    - Teste sómente com a captura de dados sem tratamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "\n",
    "# ---------------------- PEGAR LINKS COM SELENIUM ----------------------\n",
    "def coletar_links():\n",
    "    driver = webdriver.Chrome()\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "\n",
    "    driver.get(\"https://www.imdb.com/pt/\")\n",
    "\n",
    "    menu_icon = wait.until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"imdbHeader-navDrawerOpen\"]'))\n",
    "    )\n",
    "    menu_icon.click()\n",
    "\n",
    "    link_series_bem_avaliadas = wait.until(\n",
    "        EC.element_to_be_clickable((\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"imdbHeader\"]/div/aside[1]/div/div[2]/div/div[2]/div[1]/span/div/div/ul/a[2]'\n",
    "        ))\n",
    "    )\n",
    "    link_series_bem_avaliadas.click()\n",
    "\n",
    "    series = wait.until(\n",
    "        EC.presence_of_all_elements_located((\n",
    "            By.XPATH,\n",
    "            '//*[@id=\"__next\"]/main/div/div[3]/section/div/div[2]/div/ul/li'\n",
    "        ))\n",
    "    )\n",
    "\n",
    "    series_data = []\n",
    "    for serie in series:\n",
    "        try:\n",
    "            info = serie.text.split(\"\\n\")\n",
    "            link = serie.find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "            series_data.append({\"info\": info, \"link\": link})\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Erro ao extrair série: {e}\")\n",
    "\n",
    "    driver.quit()\n",
    "    return series_data\n",
    "\n",
    "# ---------------------- FUNÇÃO ASSÍNCRONA DE COLETA ----------------------\n",
    "async def fetch_and_parse(session, serie_data, idx, total):\n",
    "    try:\n",
    "        async with session.get(serie_data[\"link\"]) as response:\n",
    "            html = await response.text()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "            # Popularidade (tentando pegar o equivalente do XPath usado no Selenium)\n",
    "            popularidade_tag = soup.select_one('a[href*=\"ratings\"] span')\n",
    "            popularidade = popularidade_tag.get_text(strip=True) if popularidade_tag else \"N/A\"\n",
    "\n",
    "            # Elenco principal\n",
    "            elenco_section = soup.select(\"section[data-testid='title-cast'] li.ipc-inline-list__item\")\n",
    "            elenco_principal = [ator.get_text(\" \", strip=True) for ator in elenco_section]\n",
    "\n",
    "            logging.info(f\"Coletada {idx}/{total} - {serie_data['info'][0]}\")\n",
    "\n",
    "            return {\n",
    "                \"info\": serie_data[\"info\"],\n",
    "                \"link\": serie_data[\"link\"],\n",
    "                \"popularidade\": popularidade,\n",
    "                \"elenco_principal\": elenco_principal\n",
    "            }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erro em {serie_data['link']}: {e}\")\n",
    "        return None\n",
    "\n",
    "async def coletar_detalhes(series_data):\n",
    "    start_time = time.time()\n",
    "    total = len(series_data)\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_and_parse(session, s, idx + 1, total) for idx, s in enumerate(series_data)]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    logging.info(f\"Coleta finalizada em {elapsed:.2f} segundos.\")\n",
    "    return [r for r in results if r]\n",
    "\n",
    "# ---------------------- EXECUÇÃO NO JUPYTER ----------------------\n",
    "series_data = coletar_links()\n",
    "logging.info(f\"{len(series_data)} links coletados. Iniciando coleta assíncrona...\")\n",
    "\n",
    "# No Jupyter, usar await\n",
    "dados_completos = await coletar_detalhes(series_data)\n",
    "\n",
    "# Resultado final\n",
    "print(dados_completos)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpad_t2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
